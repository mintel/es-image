---
apiVersion: v1
data:
  10_forward.input.conf: |
    # Takes the messages sent over TCP
    <source>
      @id forward-tcp
      @type forward
    </source>
  10_kubernetes.input.conf: |
    <source>
      @id containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/es-containers.log.pos
      tag raw.kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
  10_monitoring.conf: |
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @id prometheus
      @type prometheus
    </source>

    <source>
      @id monitor_agent
      @type monitor_agent
    </source>

    # input plugin that collects metrics from MonitorAgent
    <source>
      @id prometheus_monitor
      @type prometheus_monitor
      <labels>
        host ${hostname}
        node "#{ENV['NODE_NAME']}"
      </labels>
    </source>

    # input plugin that collects metrics for output plugin
    <source>
      @id prometheus_output_monitor
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
        node "#{ENV['NODE_NAME']}"
      </labels>
    </source>

    # input plugin that collects metrics for in_tail plugin
    <source>
      @id prometheus_tail_monitor
      @type prometheus_tail_monitor
      <labels>
        host ${hostname}
        node "#{ENV['NODE_NAME']}"
      </labels>
    </source>
  10_dummy.input.conf: |
    # Generaget Dummy events
    <source>
      @id dummy
      @type dummy
      dummy {"dummy":"event"}
      rate 1
      auto_increment_key foo
      tag raw.dummy.*
    </source>
  50_kubernetes_filters.conf: |
    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @id kubernetes_metadata
      @type kubernetes_metadata
    </filter>

    <filter kubernetes.**>
      @id kubernetes_logformat
      @type record_transformer
      enable_ruby
      <record>
        logformat ${ if record.has_key? 'kubernetes' and record['kubernetes'].has_key? 'labels' and record['kubernetes']['labels'].has_key? 'logformat' ; record['kubernetes']['labels']['logformat'] ; else ; 'string' ; end }
      </record>
    </filter>

    # Re-parse Messages from containers with JSON logformat
    <match kubernetes.**>
      @id kubernetes_reroute_logformat
      @type rewrite_tag_filter
      <rule>
        key     logformat
        pattern /^json$/
        tag     JSON.${tag}
      </rule>
      <rule>
        key     logformat
        pattern /.*/
        tag     STRING.${tag}
      </rule>
    </match>

    <filter JSON.kubernetes.**>
      @id kubernetes_json_parser
      @type parser
      key_name log
      reserve_data true
      reserve_time true
      remove_key_name_field true
      inject_key_prefix json.
      <parse>
        @type multi_format
        <pattern>
          format json
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>
  59_common_late_filters.conf: |
    # Add Extra fields to records before shipping to elasticsearch
    <filter **>
      @id transform_all_add_k8s_keys
      @type record_transformer
      <record>
        k8s_host "#{ENV['NODE_NAME']}"
        k8s_cluster "cluster1"
        k8s_environment "test"
      </record>
    </filter>

    <filter **>
      @id prometheus_logging_entry_count
      @type prometheus
      <metric>
        type counter
        name logging_entry_count
        desc Total number of log entries generated by either application containers or system components
      </metric>
    </filter>

    # Do not collect fluentd's own logs to avoid infinite loops.
    # define <match fluent.**> to capture fluentd logs in top level is deprecated. Use <label @FLUENT_LOG> instead
    #<match fluent.**>
    #  @type null
    #</match>
    <label @FLUENT_LOG>
      <match fluent.*>
        @type stdout
      </match>
    </label>
  80_output.conf: |
    # @log_level info
    <match *.kubernetes.**>
      @id elasticsearch_kubernetes
      @type elasticsearch
      include_tag_key true
      host elasticsearch-data-log
      port 9200
      # Try to reload the elasticsearch node IP addresses if there is a failure making a request to it.
      reload_on_failure true
      # If a data node refuses events from fluentd - perhaps because it's overloaded - it marks it as dead for 60s
      # before trying again by default. Reducing this to 10s.
      resurrect_after 10s
      # Higher-performance JSON serialiser
      prefer_oj_serializer true

      include_timestamp true

      # ILM Settings - WITH ROLLOVER
      # https://github.com/uken/fluent-plugin-elasticsearch#enable-index-lifecycle-management
      index_name kubernetes
      rollover_index true
      deflector_alias kubernetes
      index_prefix kubernetes
      application_name default
      # If empty string("") is specified in index_date_patter, index date pattern is not used. Elasticsearch plugin just creates <index_prefix-application_name-000001> 
      #index_date_pattern "now/d"
      index_date_pattern ""
      enable_ilm true
      # Policy configurations
      ilm_policy_id kubernetes
      ilm_policy {"policy":{"phases":{"hot":{"actions":{"rollover":{"max_age":"1h","max_size":"2M"}}},"delete":{"min_age":"30d","actions":{"delete":{}}},"warm":{"actions":{"allocate":{"number_of_replicas":1},"forcemerge":{"max_num_segments":1}}}}}}
      template_name kubernetes
      template_file /etc/fluent/config.d/es-template-logstash.json
      template_overwrite true
      customize_template {"<<index_prefix>>": "kubernetes-*"}

      # NO ILM Settings - NO ROLLOVER support
      # logstash_format true
      # logstash_prefix kubernetes
      # template_name kubernetes
      # template_file /etc/fluent/config.d/es-template-logstash.json
      # template_overwrite true
      # customize_template {"<<index_prefix>>": "kubernetes-*"}


      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.elasticsearch.buffer
        flush_mode interval
        flush_thread_count 2
        flush_interval 10s
        retry_forever false
        retry_timeout 1h
        retry_type exponential_backoff
        retry_max_interval 5m
        chunk_limit_size 2M
        total_limit_size 512M
        queue_limit_length 8
        overflow_action drop_oldest_chunk
        disable_chunk_backup
      </buffer>
    </match>

    <match **>
      @id elasticsearch_catch_all
      @type elasticsearch
      @log_level info
      include_tag_key true
      host elasticsearch-data-log
      port 9200
      # Try to reload the elasticsearch node IP addresses if there is a failure making a request to it.
      reload_on_failure true
      # If a data node refuses events from fluentd - perhaps because it's overloaded - it marks it as dead for 60s
      # before trying again by default. Reducing this to 10s.
      resurrect_after 10s
      # Higher-performance JSON serialiser
      prefer_oj_serializer true

      include_timestamp true

      # ILM Settings - WITH ROLLOVER support
      # https://github.com/uken/fluent-plugin-elasticsearch#enable-index-lifecycle-management
      index_name logstash
      rollover_index true
      deflector_alias logstash
      index_prefix logstash
      application_name default
      # If empty string("") is specified in index_date_patter, index date pattern is not used. Elasticsearch plugin just creates <index_prefix-application_name-000001> 
      index_date_pattern "now/d"
      enable_ilm true
      # Policy configurations
      ilm_policy_id logstash
      ilm_policy {"policy":{"phases":{"hot":{"actions":{"rollover":{"max_age":"1h","max_size":"2M"}}},"delete":{"min_age":"30d","actions":{"delete":{}}},"warm":{"actions":{"allocate":{"number_of_replicas":1},"forcemerge":{"max_num_segments":1}}}}}}
      template_name logstash
      template_file /etc/fluent/config.d/es-template-logstash.json
      template_overwrite true
      customize_template {"<<index_prefix>>": "logstash-*"}

      <buffer>
        @type file
        path /var/log/fluentd-buffers/logstash.elasticsearch.buffer
        flush_mode interval
        flush_thread_count 2
        flush_interval 10s
        retry_forever false
        retry_timeout 1h
        retry_type exponential_backoff
        retry_max_interval 5m
        chunk_limit_size 2M
        total_limit_size 512M
        queue_limit_length 8
        overflow_action drop_oldest_chunk
        disable_chunk_backup
      </buffer>
    </match>
  es-template-logstash.json: |
    {
      "version" : 60001,
      "settings" : {
        "index.refresh_interval" : "5s",
        "number_of_shards": 1,
        "number_of_replicas": 2
      },
      "mappings" : {
        "dynamic_templates" : [
          {
            "message_field" : {
              "path_match" : "message",
              "match_mapping_type" : "string",
              "mapping" : {
                "type" : "text",
                "copy_to" : "log",
                "norms" : false
              }
            }
          },
          {
            "string_fields" : {
              "match" : "*",
              "match_mapping_type" : "string",
              "mapping" : {
                "type" : "text",
                "norms" : false,
                "copy_to" : "log",
                "fields" : {
                  "keyword" : { "type": "keyword", "ignore_above": 256 }
                }
              }
            }
          }
        ],
        "properties": {
          "@timestamp": { "type": "date" },
          "@version": { "type": "keyword" },
          "geoip": {
            "dynamic": true,
            "properties": {
              "ip": { "type": "ip" },
              "location": { "type": "geo_point" },
              "latitude": { "type": "half_float" },
              "longitude": { "type": "half_float" }
            }
          }
        }
      }
    }
  system.conf: |
    <system>
      root_dir /tmp/fluentd/
    </system>
kind: ConfigMap
metadata:
  labels:
    name: fluentd
  name: fluentd
  namespace: monitoring
