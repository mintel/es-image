---
apiVersion: v1
data:
  10_forward.input.conf: |
    # Takes the messages sent over TCP
    <source>
      @id forward-tcp
      @type forward
    </source>
  10_dummy.input.conf: |
    # Generaget Dummy events
    <source>
      @id dummy
      @type dummy
      dummy {"dummy":"event"}
      rate 1
      auto_increment_key foo
      tag raw.dummy.*
    </source>
  10_kubernetes.input.conf: |
    <source>
      @id containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/es-containers.log.pos
      tag raw.kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
  10_monitoring.conf: |
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @id prometheus
      @type prometheus
    </source>

    <source>
      @id monitor_agent
      @type monitor_agent
    </source>

    # input plugin that collects metrics from MonitorAgent
    <source>
      @id prometheus_monitor
      @type prometheus_monitor
      <labels>
        host ${hostname}
        node "#{ENV['NODE_NAME']}"
      </labels>
    </source>

    # input plugin that collects metrics for output plugin
    <source>
      @id prometheus_output_monitor
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
        node "#{ENV['NODE_NAME']}"
      </labels>
    </source>

    # input plugin that collects metrics for in_tail plugin
    <source>
      @id prometheus_tail_monitor
      @type prometheus_tail_monitor
      <labels>
        host ${hostname}
        node "#{ENV['NODE_NAME']}"
      </labels>
    </source>
  10_system.input.conf: |
    # Logs from systemd-journal for interesting services.
    <source>
      @id journald-docker
      @type systemd
      path /var/log/journal
      matches [{ "_SYSTEMD_UNIT": "docker.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-docker.pos
      </storage>
      <entry>
        fields_strip_underscores true
      </entry>
      read_from_head true
      tag docker
    </source>

    <source>
      @id journald-kubelet
      @type systemd
      path /var/log/journal
      matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-kubelet.pos
      </storage>
      <entry>
        fields_strip_underscores true
      </entry>
      read_from_head true
      tag kubelet
    </source>

    <source>
      @id journald-node-problem-detector
      @type systemd
      path /var/log/journal
      matches [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-npd.pos
      </storage>
      <entry>
        fields_strip_underscores true
      </entry>
      read_from_head true
      tag node-problem-detector
    </source>

    <source>
      @id journald-kube-container-runtime-monitor
      @type systemd
      path /var/log/journal
      matches [{ "_SYSTEMD_UNIT": "kube-container-runtime-monitor.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-kube-container-runtime-monitor.pos
      </storage>
      read_from_head true
      tag kube-container-runtime-monitor
    </source>

    <source>
      @id journald-kubelet-monitor
      @type systemd
      path /var/log/journal
      matches [{ "_SYSTEMD_UNIT": "kubelet-monitor.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-kubelet-monitor.pos
      </storage>
      read_from_head true
      tag kubelet-monitor
    </source>

    <source>
      @id journald-kernel
      @type systemd
      path /var/log/journal
      matches [{ "_TRANSPORT": "kernel" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-kernel.pos
      </storage>
      <entry>
        fields_strip_underscores true
        fields_lowercase true
      </entry>
      read_from_head true
      tag kernel
    </source>

    <source>
      @id journald-sshd
      @type systemd
      path /var/log/journal
      matches [{ "SYSLOG_IDENTIFIER": "sshd" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-sshd.pos
      </storage>
      <entry>
        fields_strip_underscores true
        fields_lowercase true
      </entry>
      read_from_head true
      tag sshd
    </source>

    <source>
      @id journald-systemd-logind
      @type systemd
      path /var/log/journal
      matches [{ "_SYSTEMD_UNIT": "systemd-logind.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-logind.pos
      </storage>
      <entry>
        fields_strip_underscores true
        fields_lowercase true
      </entry>
      read_from_head true
      tag systemd-logind
    </source>

    <source>
      @id journald-systemd-timesyncd
      @type systemd
      path /var/log/journal
      matches [{ "_SYSTEMD_UNIT": "systemd-timesyncd.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-timesyncd.pos
      </storage>
      <entry>
        fields_strip_underscores true
        fields_lowercase true
      </entry>
      read_from_head true
      tag systemd-timesyncd
    </source>

    <source>
      @id journald-systemd-networkd
      @type systemd
      path /var/log/journal
      matches [{ "UNIT": "systemd-networkd.service" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-networkd.pos
      </storage>
      <entry>
        fields_strip_underscores true
        fields_lowercase true
      </entry>
      read_from_head true
      tag systemd-networkd
    </source>

    <source>
      @id journald-systemd-audit
      @type systemd
      path /var/log/journal
      matches [{ "SYSLOG_IDENTIFIER": "audit" }]
      <storage>
        @type local
        persistent true
        path /var/log/journald-audit.pos
      </storage>
      <entry>
        fields_strip_underscores true
        fields_lowercase true
      </entry>
      read_from_head true
      tag auditlogs
    </source>
  50_kubernetes_filters.conf: |
    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @id kubernetes_metadata
      @type kubernetes_metadata
    </filter>

    <filter kubernetes.**>
      @id kubernetes_logformat
      @type record_transformer
      enable_ruby
      <record>
        logformat ${ if record.has_key? 'kubernetes' and record['kubernetes'].has_key? 'labels' and record['kubernetes']['labels'].has_key? 'logformat' ; record['kubernetes']['labels']['logformat'] ; else ; 'string' ; end }
      </record>
    </filter>

    # Re-parse Messages from containers with JSON logformat
    <match kubernetes.**>
      @id kubernetes_reroute_logformat
      @type rewrite_tag_filter
      <rule>
        key     logformat
        pattern /^json$/
        tag     JSON.${tag}
      </rule>
      <rule>
        key     logformat
        pattern /.*/
        tag     STRING.${tag}
      </rule>
    </match>

    <filter JSON.kubernetes.**>
      @id kubernetes_json_parser
      @type parser
      key_name log
      reserve_data true
      reserve_time true
      remove_key_name_field true
      inject_key_prefix json.
      <parse>
        @type multi_format
        <pattern>
          format json
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>
  59_common_late_filters.conf: |
    # Add Extra fields to records before shipping to elasticsearch
    <filter **>
      @id transform_all_add_k8s_keys
      @type record_transformer
      <record>
        k8s_host "#{ENV['NODE_NAME']}"
        k8s_cluster "cluster1"
        k8s_environment "test"
      </record>
    </filter>

    <filter **>
      @id prometheus_logging_entry_count
      @type prometheus
      <metric>
        type counter
        name logging_entry_count
        desc Total number of log entries generated by either application containers or system components
      </metric>
    </filter>

     # Do not collect fluentd's own logs to avoid infinite loops.
     # define <match fluent.**> to capture fluentd logs in top level is deprecated. Use <label @FLUENT_LOG> instead
    <match fluent.**>
      @type null
    </match>
  80_output.conf: |
    # @log_level info
    <match *.kubernetes.**>
      @id elasticsearch_kubernetes
      @type elasticsearch
      include_tag_key true
      host elasticsearch-data-log
      port 9200
      logstash_format true
      logstash_prefix kubernetes
      template_name kubernetes
      template_file /etc/fluent/config.d/es-template-logstash.json
      template_overwrite true
      # Try to reload the elasticsearch node IP addresses if there is a failure making a request to it.
      reload_on_failure true
      # If a data node refuses events from fluentd - perhaps because it's overloaded - it marks it as dead for 60s
      # before trying again by default. Reducing this to 10s.
      resurrect_after 10s
      # Higher-performance JSON serialiser
      prefer_oj_serializer true

      # ILM Settings
      enable_ilm true
      deflector_alias kubernetes
      rollover_index false
      index_prefix kubernetes
      # Policy configurations
      ilm_policy_id kubernetes
      ilm_policy {"policy":{"phases":{"warm":{"min_age":"1d","actions":{"allocate":{"number_of_replicas":1}}},"delete":{"min_age":"30d","actions":{"delete":{}}}}}}

      <buffer>
        @type file
        path /tmp/fluentd-buffers/kubernetes.elasticsearch.buffer
        flush_mode interval
        flush_thread_count 2
        flush_interval 10s
        retry_forever false
        retry_timeout 1h
        retry_type exponential_backoff
        retry_max_interval 5m
        chunk_limit_size 2M
        total_limit_size 256M
        queue_limit_length 8
        overflow_action drop_oldest_chunk
        disable_chunk_backup
      </buffer>
    </match>

    <match **>
      @id elasticsearch_catch_all
      @type elasticsearch
      @log_level info
      include_tag_key true
      host elasticsearch-data-log
      port 9200
      logstash_format true
      template_name logstash
      template_file /etc/fluent/config.d/es-template-logstash.json
      template_overwrite true
      # Try to reload the elasticsearch node IP addresses if there is a failure making a request to it.
      reload_on_failure true
      # If a data node refuses events from fluentd - perhaps because it's overloaded - it marks it as dead for 60s
      # before trying again by default. Reducing this to 10s.
      resurrect_after 10s
      # Higher-performance JSON serialiser
      prefer_oj_serializer true

      # ILM Settings
      enable_ilm true
      deflector_alias logstash
      rollover_index false
      index_prefix logstash
      # Policy configurations
      ilm_policy_id logstash
      ilm_policy {"policy":{"phases":{"warm":{"min_age":"1d","actions":{"allocate":{"number_of_replicas":1}}},"delete":{"min_age":"30d","actions":{"delete":{}}}}}}

      <buffer>
        @type file
        path /tmp/fluentd-buffers/logstash.elasticsearch.buffer
        flush_mode interval
        flush_thread_count 2
        flush_interval 10s
        retry_forever false
        retry_timeout 1h
        retry_type exponential_backoff
        retry_max_interval 5m
        chunk_limit_size 2M
        total_limit_size 256M
        queue_limit_length 8
        overflow_action drop_oldest_chunk
        disable_chunk_backup
      </buffer>
    </match>
  es-template-logstash.json: |
    {
      "index_patterns" : "logstash-*",
      "settings" : {
        "index.refresh_interval" : "5s"
      },
      "mappings" : {
        "dynamic_templates" : [
          {
            "message_field" : {
              "path_match" : "message",
              "match_mapping_type" : "string",
              "mapping" : {
                "type" : "text",
                "copy_to" : "log",
                "norms" : false
              }
            }
          },
          {
            "string_fields" : {
              "match" : "*",
              "match_mapping_type" : "string",
              "mapping" : {
                "type" : "text",
                "norms" : false,
                "copy_to" : "log",
                "fields" : {
                  "keyword" : { "type": "keyword", "ignore_above": 256 }
                }
              }
            }
          }
        ],
        "properties": {
          "@timestamp": { "type": "date" },
          "@version": { "type": "keyword" },
          "geoip": {
            "dynamic": true,
            "properties": {
              "ip": { "type": "ip" },
              "location": { "type": "geo_point" },
              "latitude": { "type": "half_float" },
              "longitude": { "type": "half_float" }
            }
          }
        }
      }
    }
  system.conf: |
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>
kind: ConfigMap
metadata:
  labels:
    name: fluentd
  name: fluentd
  namespace: monitoring
